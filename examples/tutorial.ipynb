{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4dee92-ee4a-42b3-a616-844800ddc662",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mobile-env\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aaa15d-550a-40fa-858e-7b3b7dbeaed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import mobile_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd097a51-e82b-49a2-8c5f-65a064ffbde3",
   "metadata": {},
   "source": [
    "This Google Colaboratory notebook gives an introduction on how to use *mobile-env* for training & evaluating multi-agent and central decision making policies for cell selection in mobile communication settings. First, we train a multi-agent policy with **RLlib**. Second, we train a central policy with **stable-baselines3**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413e1df5-1374-442a-ae6a-87303e05a746",
   "metadata": {},
   "source": [
    "# Multi-Agent Control Setting with RLlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226d3a42-5dbb-47c2-b3e2-6ff0731a1c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U ray[rllib]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527b006d-5584-43b0-8a1c-9eeebeffbbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "\n",
    "stop = {\n",
    "    \"episodes_total\": 2000\n",
    "}\n",
    "\n",
    "config = {\n",
    "        # enviroment configuration:\n",
    "        \"env\": \"mobile-small-ma-v0\",\n",
    "\n",
    "        # agent configuration:\n",
    "        \"multiagent\": {\n",
    "            \"policies\": {\"shared_policy\"},\n",
    "            \"policy_mapping_fn\": (\n",
    "                lambda agent_id, **kwargs: \"shared_policy\"),\n",
    "        },\n",
    "}\n",
    "save_dir = \".\"\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(\n",
    "  num_cpus=3,\n",
    "  include_dashboard=False,\n",
    "  ignore_reinit_error=True,\n",
    "  log_to_driver=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d055b48-a34f-4a8e-9aa3-6b2300eb0207",
   "metadata": {},
   "source": [
    "To use multi-agent policies of RLlib, we must first register our custom OpenAI Gym environment. The RLlibMAWrapper class can be used to wrap the default multi-agent simulation so that it conforms with RLlib's MultiAgentEnv. Now, the environment defines an action and observation space for each user equipment (UE), attributes rewards per UE (per agent) and returns partial observations (no global knowledge for agents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090762b7-1ab4-4f12-8146-a4153677e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "\n",
    "\n",
    "def register(config):\n",
    "    import mobile_env\n",
    "    from mobile_env.wrappers.multi_agent import RLlibMAWrapper\n",
    "    env = gym.make(\"mobile-small-ma-v0\")\n",
    "    return RLlibMAWrapper(env)\n",
    "\n",
    "register_env(\"mobile-small-ma-v0\", register)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35881c26-4a9a-4051-86d5-8b504f7a624c",
   "metadata": {},
   "source": [
    "Run RLlib (this can take a while):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c7a216-3e69-4c37-a443-f92cb79665d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = ray.tune.run(ppo.PPOTrainer, config=config, local_dir=save_dir, stop=stop, checkpoint_at_end=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52162d3b-5e07-4c71-a86c-bd01cdfdf37c",
   "metadata": {},
   "source": [
    "Visualize the training with Tensorboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4069ab5-1ca5-4965-9e70-92dd70080105",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ray_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c32f8b-eefd-4c30-808d-6bfb4bbbee35",
   "metadata": {},
   "source": [
    "To visualize the final multi-agent policy, load the latest model checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc933bc-4983-49ba-a7f4-8032886bd045",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = analysis.get_last_checkpoint()\n",
    "model = ppo.PPOTrainer(config=config, env='mobile-small-ma-v0')\n",
    "model.restore(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cff06a-46bb-4634-98fb-9386ef30dca2",
   "metadata": {},
   "source": [
    "Mobile-Env provides a render() function to visualize the simulation. In Google Colaboratory the better-looking 'human' mode is unavailable (only available locally). Still, we can visualize the final policy as RGB images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0954673-504d-4d1a-91d0-9834f4673754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "env = register('mobile-small-ma-v0')\n",
    "done = {'__all__': False}\n",
    "obs = env.reset()\n",
    "\n",
    "while not done['__all__']:\n",
    "    # gather action from each actor (for each UE)\n",
    "    action = {}\n",
    "    for agent_id, agent_obs in obs.items():\n",
    "        policy_id = config['multiagent']['policy_mapping_fn'](agent_id)\n",
    "        action[agent_id] = model.compute_action(agent_obs, policy_id=policy_id)\n",
    "    \n",
    "    # perform step on simulation environment \n",
    "    obs, reward, done, info = env.step(action)\n",
    "\n",
    "    # display environment as RGB\n",
    "    plt.imshow(env.env.render(mode='rgb_array'))\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    time.sleep(0.025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ab9204-6fe7-4bdc-a1fd-8448ba352107",
   "metadata": {},
   "source": [
    "# Central Environment with Stable-Baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f634160-6737-4a8a-9848-d31ea202d79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stable-baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2d6df8-b020-4e85-80fe-7c9572fb9fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349c7adf-d971-442b-a5b9-a82b40eca351",
   "metadata": {},
   "source": [
    "The decision making policy can also be trained on scenarios that simulate centralized control over user equipments, i.e., one single agent decides what connections should be established for each UE and is given global information. The centralized setting wraps the observations, rewards and actions of the multi-agent setting. Now, observations are single (concatenated) vectors that jointly represent up-to-date information on all UEs. The reward is the average utility of active UEs. Similarly, actions are vectors of discrete decisions.\n",
    "\n",
    "Use *stable-baselines3*'s PPO agent on the (small) centralized control environment and train it for 500,000 steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490a6382-a5c2-40cf-9f24-431c4a3e796c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the small central simulation\n",
    "env = gym.make(\"mobile-small-central-v0\")\n",
    "\n",
    "# train PPO agent on environment\n",
    "model = PPO(MlpPolicy, env, verbose=1, tensorboard_log='ppo_central_tensorboard')\n",
    "model.learn(total_timesteps=500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcd0c7e-1725-4d5e-8e3d-29560eff0e39",
   "metadata": {},
   "source": [
    "Visualize the training results with Tensorboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e544681c-938e-45eb-9481-5d8084c9028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ppo_central_tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db72f60-2297-4a70-bc5c-04fc07622895",
   "metadata": {},
   "source": [
    "Visualize what the central agent has learned. Note that the RGB visualization on Google Colaboratory does not render the environment as clearly as the 'human' mode, which is unavailable in virtual environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f251ed-a1ef-4647-abc1-9457cbd4d72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make('mobile-small-central-v0')\n",
    "done = False\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action, _ = model.predict(obs)\n",
    "\n",
    "    # perform step on simulation environment \n",
    "    obs, reward, done, info = env.step(action)\n",
    "\n",
    "    # display environment as RGB\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    time.sleep(0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1297e5-b221-49d7-ba51-361b860e22b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mobile",
   "language": "python",
   "name": "mobile"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
